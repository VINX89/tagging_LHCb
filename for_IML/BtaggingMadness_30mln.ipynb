{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Немного безумия\n",
    "\n",
    "смотрим, работают ли attention models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import root_numpy\n",
    "# import pandas - no pandas today \n",
    "from astropy.table import Table\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# theano imports\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.tensor.nnet import softplus\n",
    "from theano.tensor.extra_ops import bincount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    # track itself\n",
    "    'eta', 'partPt', 'partP',\n",
    "    # track and B\n",
    "    'cos_diff_phi', 'proj', 'diff_eta', 'ptB', 'R_separation', 'proj_T', 'proj_T2',\n",
    "    # PID\n",
    "    'PIDNNe',  'PIDNNk',  'PIDNNm', 'ghostProb', \n",
    "    # IP\n",
    "    'IP', 'IPerr', 'IPs', 'IPPU', \n",
    "    # Other\n",
    "    'veloch', 'partlcs', 'EOverP', \n",
    "    # deleted as probably inappropriate:\n",
    "    # 'phi',  \n",
    "    # 'diff_pt', 'nnkrec',\n",
    "    # 'max_PID_mu_e', 'max_PID_mu_k', 'sum_PID_k_e', 'sum_PID_mu_e', 'max_PID_k_e', 'sum_PID_mu_k', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = Table(root_numpy.root2array('../datasets/MC/csv/WG/Bu_JPsiK/2012/Tracks.root', stop=3000))\n",
    "data = Table(root_numpy.root2array('../datasets/MC/csv/WG/Bu_JPsiK/2012/Tracks.root', stop=30000000))\n",
    "\n",
    "data['label'] = (data['signB'] * data['signTrack']) > 0\n",
    "\n",
    "data['cos_diff_phi'] = numpy.cos(data['diff_phi'])\n",
    "data['diff_pt'] = data['ptB'] - data['partPt']\n",
    "data['R_separation'] = numpy.sqrt(data['diff_eta'] ** 2 + (1 - data['cos_diff_phi']) ** 2)\n",
    "# projection in transverse plane\n",
    "data['proj_T'] = data['cos_diff_phi'] * data['partPt']\n",
    "data['proj_T2'] = data['cos_diff_phi'] * data['partPt'] * data['ptB']\n",
    "\n",
    "data = data[data['ghostProb'] < 0.4]\n",
    "data = data[numpy.isfinite(data['IPs'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groups = data['run'] + data['event'].astype(int) * (data['run'].max() + 1)\n",
    "_, data['group_column'] = numpy.unique(groups, return_inverse=True)\n",
    "data = data.group_by('group_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_weights(data, attention):\n",
    "    \"\"\"\n",
    "    Weights are normalized over events. Higher convenience - higher weights\n",
    "    \"\"\"\n",
    "    assert len(numpy.shape(attention)) == 1\n",
    "    weights = numpy.exp(attention)\n",
    "    sum_weights = numpy.bincount(data['group_column'], weights=weights)\n",
    "    return weights / (sum_weights[data['group_column']] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_auc_with_attention(data, track_proba, track_attention):\n",
    "    assert track_proba.shape == (len(data), 2)\n",
    "    assert len(track_attention) == len(data)\n",
    "    tracks_weights = compute_weights(data, track_attention)\n",
    "    event_predictions = numpy.bincount(\n",
    "        data['group_column'], weights=logit(track_proba[:, 1]) * data['signTrack'] * tracks_weights)\n",
    "    B_signs = data['signB'].group_by(data['group_column']).groups.aggregate(numpy.mean)\n",
    "    return roc_auc_score(B_signs, event_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Просто поезд + inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/mfs/miniconda/envs/rep_py2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from folding_group import FoldingGroupClassifier\n",
    "from decisiontrain import DecisionTrainClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_clf = DecisionTrainClassifier(n_estimators=1000, learning_rate=0.03, n_threads=len(features), \n",
    "                                   train_features=features, max_features=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 53s, sys: 8min 24s, total: 54min 18s\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "_ = dt.fit(data[features + ['group_column']].to_pandas(), data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66057126874375682"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw quality\n",
    "B_signs = data['signB'].group_by(data['group_column']).groups.aggregate(numpy.mean)\n",
    "track_proba = dt.predict_proba(data.to_pandas())\n",
    "predictions = numpy.bincount(data['group_column'], weights=logit(track_proba[:, 1]) * data['signTrack'])\n",
    "roc_auc_score(B_signs, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66281719228620928"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data, \n",
    "                           track_proba=dt.predict_proba(data.to_pandas()), \n",
    "                           track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66281719228620928"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data, \n",
    "                           track_proba=dt.predict_proba(data.to_pandas()), \n",
    "                           track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47min 23s, sys: 9min 8s, total: 56min 31s\n",
      "Wall time: 9min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_n_tracks = numpy.bincount(data['group_column'])[data['group_column']] \n",
    "_weights = (_n_tracks > 5) & (_n_tracks < 40)\n",
    "dt_on_filtered = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "_ = dt_on_filtered.fit(data[features + ['group_column']].to_pandas(), data['label'], sample_weight=_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66283443034982881"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on filtered dataset\n",
    "compute_auc_with_attention(data, \n",
    "                           track_proba=dt_on_filtered.predict_proba(data.to_pandas()),\n",
    "                           track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.010999999999999999, 'eta'),\n",
       " (0.010999999999999999, 'partlcs'),\n",
       " (0.035000000000000003, 'diff_eta'),\n",
       " (0.037000000000000005, 'proj_T2'),\n",
       " (0.037999999999999999, 'cos_diff_phi'),\n",
       " (0.037999999999999999, 'ptB'),\n",
       " (0.044999999999999998, 'IPerr'),\n",
       " (0.047, 'veloch'),\n",
       " (0.055999999999999994, 'EOverP'),\n",
       " (0.056000000000000001, 'ghostProb'),\n",
       " (0.059999999999999998, 'IPPU'),\n",
       " (0.066000000000000003, 'partP'),\n",
       " (0.072000000000000008, 'proj_T'),\n",
       " (0.08299999999999999, 'R_separation'),\n",
       " (0.109, 'PIDNNe'),\n",
       " (0.13800000000000001, 'PIDNNm'),\n",
       " (0.189, 'IP'),\n",
       " (0.20499999999999999, 'IPs'),\n",
       " (0.215, 'proj'),\n",
       " (0.23399999999999999, 'PIDNNk'),\n",
       " (0.255, 'partPt')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(dt.estimators[0].feature_importances_ + dt.estimators[1].feature_importances_, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.010999999999999999, 'eta'),\n",
       " (0.010999999999999999, 'partlcs'),\n",
       " (0.035000000000000003, 'diff_eta'),\n",
       " (0.037000000000000005, 'proj_T2'),\n",
       " (0.037999999999999999, 'cos_diff_phi'),\n",
       " (0.037999999999999999, 'ptB'),\n",
       " (0.044999999999999998, 'IPerr'),\n",
       " (0.047, 'veloch'),\n",
       " (0.055999999999999994, 'EOverP'),\n",
       " (0.056000000000000001, 'ghostProb'),\n",
       " (0.059999999999999998, 'IPPU'),\n",
       " (0.066000000000000003, 'partP'),\n",
       " (0.072000000000000008, 'proj_T'),\n",
       " (0.08299999999999999, 'R_separation'),\n",
       " (0.109, 'PIDNNe'),\n",
       " (0.13800000000000001, 'PIDNNm'),\n",
       " (0.189, 'IP'),\n",
       " (0.20499999999999999, 'IPs'),\n",
       " (0.215, 'proj'),\n",
       " (0.23399999999999999, 'PIDNNk'),\n",
       " (0.255, 'partPt')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(dt.estimators[0].feature_importances_ + dt.estimators[1].feature_importances_, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Длинный поезд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "long_dt = FoldingGroupClassifier(\n",
    "    DecisionTrainClassifier(n_estimators=3000, learning_rate=0.02, max_features=0.9,\n",
    "                            n_threads=len(features), train_features=features), \n",
    "    n_folds=2, group_feature='group_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = long_dt.fit(data[features + ['group_column']].to_pandas(), \n",
    "                data['label'], \n",
    "                sample_weight=compute_weights(data, numpy.zeros(len(data)) - 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n",
      "0.649884278779\n",
      "0.660581502153\n",
      "0.662401725219\n",
      "0.663233151088\n",
      "0.663442556187\n",
      "0.663394922291\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(long_dt.staged_predict_proba(data.to_pandas()), 1):\n",
    "    if i % 5 == 0:\n",
    "        print compute_auc_with_attention(data, track_proba=p, track_attention=numpy.zeros(len(data)) - 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Если просто взвесить обучающую выборку\n",
    "\n",
    "то есть отнормировать веса треков внутри одного события, получаем существенный прирост, если их использовать во время предсказания.\n",
    "\n",
    "При этом использование весов в тренировке - это странно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = compute_weights(data, attention=numpy.zeros(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_simpleweights = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "dt_simpleweights.fit(data[features + ['group_column']].to_pandas(), data['label'], sample_weight=weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66260082020698474"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data, \n",
    "                           track_proba=dt_simpleweights.predict_proba(data.to_pandas()), \n",
    "                           track_attention=numpy.zeros(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посмотрим на качество в зависимости от числа треков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tracks = numpy.bincount(data['group_column'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fab13ffe7d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEQCAYAAAAUILtVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab2ca346d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(numpy.bincount(n_tracks) * numpy.arange(max(n_tracks) + 1))\n",
    "plt.xlabel('n_tracks in event')\n",
    "plt.ylabel('n_tracks in total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(* zip(*(\n",
    "#         [i, roc_auc_score(B_signs[n_tracks == i], predictions[n_tracks == i])]\n",
    "#         for i in range(2, 60)\n",
    "# )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытаемся отличить правильно предсказанные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correctness = logit(track_proba[:, 1]) * (2 * data['label'] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n",
      "50 KFold prediction using folds column\n",
      "0.661690653583\n",
      "KFold prediction using folds column\n",
      "60 KFold prediction using folds column\n",
      "0.662061552908\n",
      "KFold prediction using folds column\n",
      "70 KFold prediction using folds column\n",
      "0.660748379403\n",
      "KFold prediction using folds column\n",
      "80 KFold prediction using folds column\n",
      "0.658654955968\n"
     ]
    }
   ],
   "source": [
    "for percentile in [50, 60, 70, 80]:\n",
    "    dt_attention = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "    dt_attention.fit(data[features + ['group_column']].to_pandas(), \n",
    "                     correctness > numpy.percentile(correctness, percentile))\n",
    "\n",
    "    attention = logit(dt_attention.predict_proba(data[features + ['group_column']].to_pandas())[:, 1])\n",
    "    if percentile == 70:\n",
    "        stable_attention = attention.copy()\n",
    "    attention_weights = compute_weights(data, attention)\n",
    "\n",
    "    dt_classifier = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "    dt_classifier.fit(data[features + ['group_column']].to_pandas(), \n",
    "                      data['label'], sample_weight=attention_weights)\n",
    "\n",
    "    print percentile, compute_auc_with_attention(data, dt_classifier.predict_proba(data.to_pandas()), attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная проверка верности корректировки веса\n",
    "пробуем, работает ли линейная корректировка веса.\n",
    "\n",
    "Да, при этом сдвиг является важным, то есть наличие нормализации порядка $e^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n",
      "-3 \t0.5 \t0.661327839454\n",
      "-3 \t0.675 \t0.660745030809\n",
      "-3 \t0.85 \t0.660142538988\n",
      "-3 \t1.025 \t0.659531619479\n",
      "-3 \t1.2 \t0.658919313248\n",
      "-2 \t0.5 \t0.661714029974\n",
      "-2 \t0.675 \t0.661178247078\n",
      "-2 \t0.85 \t0.660612026943\n",
      "-2 \t1.025 \t0.660030440896\n",
      "-2 \t1.2 \t0.65944330229\n",
      "-1 \t0.5 \t0.661618636182\n",
      "-1 \t0.675 \t0.661125887542\n",
      "-1 \t0.85 \t0.660594533625\n",
      "-1 \t1.025 \t0.660042541154\n",
      "-1 \t1.2 \t0.659482073589\n",
      "0 \t0.5 \t0.661381131178\n",
      "0 \t0.675 \t0.660903309288\n",
      "0 \t0.85 \t0.660383418451\n",
      "0 \t1.025 \t0.659840838786\n",
      "0 \t1.2 \t0.659288848443\n"
     ]
    }
   ],
   "source": [
    "_proba = dt.predict_proba(data.to_pandas())\n",
    "for alpha in [-3, -2, -1, 0]:\n",
    "    for beta in numpy.linspace(0.5, 1.2, 5):\n",
    "        print alpha, '\\t', beta, '\\t', compute_auc_with_attention(data, _proba, alpha + beta * stable_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная  проверка \n",
    "того, что взаимное обучение attention и классификатора работает. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.662609360691\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660602583854\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660905334811\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660813626497\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660784447699\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660819333451\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660824832463\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660780437217\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660655642292\n",
      "KFold prediction using folds column\n",
      "KFold prediction using folds column\n",
      "0.660558555326\n"
     ]
    }
   ],
   "source": [
    "# lazy start\n",
    "_attention = numpy.zeros(len(data))\n",
    "_correctness = numpy.zeros(len(data))\n",
    "\n",
    "for iteration in range(5):\n",
    "    dt_classifier = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "    dt_classifier.fit(data[features + ['group_column']].to_pandas(),\n",
    "                      data['label'], sample_weight=compute_weights(data, _attention))\n",
    "    \n",
    "    _correctness = logit(dt_classifier.predict_proba(data.to_pandas())[:, 1]) * (2 * data['label'] - 1)\n",
    "    \n",
    "    print compute_auc_with_attention(data, dt_classifier.predict_proba(data.to_pandas()), _attention)\n",
    "    \n",
    "    dt_attention = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "    dt_attention.fit(data[features + ['group_column']].to_pandas(),\n",
    "                     _correctness > numpy.percentile(_correctness, 70))\n",
    "    \n",
    "    _attention = logit(dt_attention.predict_proba(data.to_pandas())[:, 1])\n",
    "    \n",
    "    print compute_auc_with_attention(data, dt_classifier.predict_proba(data.to_pandas()), _attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная проверка \n",
    "что обучение с приоритезацией работает. Оставялем везде по два-три трека\n",
    "\n",
    "- не работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO fast orders\n",
    "# from astropy.table import Column\n",
    "\n",
    "# def compute_orders(indices, predictions):\n",
    "#     '''less is better'''\n",
    "#     assert len(indices) == len(predictions)\n",
    "#     result = numpy.zeros(len(predictions))\n",
    "#     for index in numpy.unique(indices):\n",
    "#         result[indices == index] = numpy.argsort(numpy.argsort( - predictions[indices == index]))\n",
    "#     return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# orders = compute_orders(data['group_column'], correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for percentile in [60, 70, 80]:\n",
    "#     for n_tracks in [3, 5, 7]:\n",
    "#         # lazy start\n",
    "#         _attention = numpy.zeros(len(data))\n",
    "\n",
    "#         dt_classifier = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "#         dt_classifier.fit(data[features + ['group_column']].to_pandas(),\n",
    "#                           data['label'], sample_weight=compute_weights(data, _attention))\n",
    "\n",
    "#         _correctness = logit(dt_classifier.predict_proba(data.to_pandas())[:, 1]) * (2 * data['label'] - 1)\n",
    "\n",
    "#         dt_attention = FoldingGroupClassifier(base_clf, n_folds=2, group_feature='group_column')\n",
    "#         dt_attention.fit(data[features + ['group_column']].to_pandas(),\n",
    "#                          (_correctness > numpy.percentile(_correctness, percentile)) * (orders < n_tracks) )\n",
    "\n",
    "#         _attention = logit(dt_attention.predict_proba(data.to_pandas())[:, 1])\n",
    "\n",
    "#         print n_tracks, '\\t', percentile, '\\t', \\\n",
    "#             compute_auc_with_attention(data, dt_classifier.predict_proba(data.to_pandas()), _attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшое сравнение importance для моделей attention и classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0030000000000000001, 'eta'),\n",
       " (0.0030000000000000001, 'partlcs'),\n",
       " (0.0070000000000000001, 'diff_eta'),\n",
       " (0.014999999999999999, 'proj_T2'),\n",
       " (0.017999999999999999, 'cos_diff_phi'),\n",
       " (0.02, 'IPPU'),\n",
       " (0.021000000000000001, 'veloch'),\n",
       " (0.023, 'EOverP'),\n",
       " (0.023, 'ghostProb'),\n",
       " (0.023, 'ptB'),\n",
       " (0.029000000000000001, 'IPerr'),\n",
       " (0.029000000000000001, 'partP'),\n",
       " (0.043999999999999997, 'proj_T'),\n",
       " (0.044999999999999998, 'R_separation'),\n",
       " (0.049000000000000002, 'PIDNNe'),\n",
       " (0.085000000000000006, 'PIDNNm'),\n",
       " (0.087999999999999995, 'IP'),\n",
       " (0.098000000000000004, 'IPs'),\n",
       " (0.115, 'proj'),\n",
       " (0.128, 'PIDNNk'),\n",
       " (0.13400000000000001, 'partPt')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(dt_classifier.estimators[0].feature_importances_, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 'partlcs'),\n",
       " (0.001, 'cos_diff_phi'),\n",
       " (0.0040000000000000001, 'eta'),\n",
       " (0.014, 'veloch'),\n",
       " (0.014999999999999999, 'diff_eta'),\n",
       " (0.017000000000000001, 'ghostProb'),\n",
       " (0.017000000000000001, 'proj_T2'),\n",
       " (0.019, 'IPPU'),\n",
       " (0.024, 'IPerr'),\n",
       " (0.024, 'PIDNNm'),\n",
       " (0.028000000000000001, 'partP'),\n",
       " (0.032000000000000001, 'EOverP'),\n",
       " (0.033000000000000002, 'R_separation'),\n",
       " (0.042999999999999997, 'ptB'),\n",
       " (0.047, 'proj_T'),\n",
       " (0.059999999999999998, 'IPs'),\n",
       " (0.063, 'PIDNNe'),\n",
       " (0.12, 'partPt'),\n",
       " (0.123, 'PIDNNk'),\n",
       " (0.156, 'proj'),\n",
       " (0.16, 'IP')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(dt_attention.estimators[0].feature_importances_, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Математическая модель дальнейших действий\n",
    "\n",
    "$$ d(B+) = \\sum_{\\text{track}} d(B+ | \\text{track is tagging}) 1_\\text{track is tagging} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(b_signs, event_indices, d_tracksign, d_is_tagging):\n",
    "    track_contributions = T.exp(d_is_tagging)\n",
    "    track_contributions /= (T.extra_ops.bincount(event_indices, track_contributions) + 1)[event_indices]\n",
    "    d_B = bincount(event_indices, d_tracksign * track_contributions)\n",
    "    return T.mean(T.exp(- b_signs * d_B))\n",
    "    # return T.mean(b_signs * d_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class AttentionLoss(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # self.sample_weight = numpy.require(sample_weight, dtype='float32')\n",
    "        self.sample_weight = numpy.ones(len(X))\n",
    "        self.y_signed = numpy.require(2 * y - 1, dtype='float32')\n",
    "        \n",
    "        _, first_positions, event_indices = numpy.unique(X['group_column'].values, return_index=True, return_inverse=True)\n",
    "        self.b_signs = numpy.array(X['signB'].values)[first_positions]\n",
    "        \n",
    "        d_is_tagging_var = T.vector()\n",
    "        self.Loss = loss_function(\n",
    "            self.b_signs, event_indices, \n",
    "            d_tracksign=numpy.array(X['trackpredictions'].values),\n",
    "            d_is_tagging=d_is_tagging_var\n",
    "        )\n",
    "        self.grad = theano.function([d_is_tagging_var], - T.grad(self.Loss, d_is_tagging_var))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def prepare_tree_params(self, pred):\n",
    "        _grad = numpy.sign(self.grad(pred))\n",
    "        return _grad / numpy.std(_grad), self.sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    }
   ],
   "source": [
    "data2 = data.copy()\n",
    "data2['trackpredictions'] = logit(dt.predict_proba(data.to_pandas())[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _base = DecisionTrainClassifier(loss=AttentionLoss(), n_estimators=1000, \n",
    "#                                 learning_rate=0.03, n_threads=16, train_features=features)\n",
    "\n",
    "# _features = features + ['group_column', 'signB', 'trackpredictions']\n",
    "# dt_grouping = FoldingGroupClassifier(_base, n_folds=2, group_feature='group_column', train_features=_features)\n",
    "\n",
    "# dt_grouping.fit(data2[_features].to_pandas(), \n",
    "#                 data2['label']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attention = logit(dt_grouping.predict_proba(data2.to_pandas())[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rep.metaml import FoldingRegressor\n",
    "from decisiontrain import DecisionTrainRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_weights = compute_weights(data, attention=numpy.zeros(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пробуем тренировку на перцентиль. Пока что самый стабильный вариант\n",
    "# folding = FoldingRegressor(DecisionTrainRegressor(n_estimators=1000))\n",
    "# folding.fit(data2[features].to_pandas(), correctness > numpy.percentile(correctness, 70), sample_weight=simple_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Тренировка на корректность работает плохо\n",
    "# folding = FoldingRegressor(DecisionTrainRegressor(n_estimators=1000))\n",
    "# folding.fit(data2[features].to_pandas(), correctness / numpy.std(correctness), sample_weight=simple_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Пробуем тренировку на порядок. Лучше, чем ничего, хуже, чем перцентиль\n",
    "# folding = FoldingRegressor(DecisionTrainRegressor(n_estimators=1000))\n",
    "# folding.fit(data2[features].to_pandas(), 1.4 ** - orders, sample_weight=simple_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Пробуем тренировку на ranktransform от \n",
    "folding = FoldingRegressor(DecisionTrainRegressor(n_estimators=1000))\n",
    "folding.fit(data2[features].to_pandas(), \n",
    "            numpy.argsort(numpy.argsort(correctness)) / float(len(correctness)) - 0.5, \n",
    "            sample_weight=simple_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Пробуем тренировку на знак\n",
    "# folding = FoldingRegressor(DecisionTrainRegressor(n_estimators=1000))\n",
    "# folding.fit(data2[features].to_pandas(), numpy.sign(correctness), sample_weight=simple_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    }
   ],
   "source": [
    "attention = folding.predict(data2.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEGCAYAAACAd+UpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fab1e823850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(attention, bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66199750556606518"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data, track_proba=dt.predict_proba(data2.to_pandas()), \n",
    "                           track_attention=attention - 1.5\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66273768778635234"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data, track_proba=dt.predict_proba(data2.to_pandas()), \n",
    "                           track_attention=compute_weights(data, attention) - 2\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66267978988955112"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data, track_proba=dt.predict_proba(data2.to_pandas()), track_attention=attention * 0 - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.66032964554767171"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_auc_with_attention(data[:10**6], \n",
    "                           track_proba=dt.predict_proba(data2.to_pandas())[:10 ** 6], \n",
    "                           track_attention=attention[:10 ** 6] * 0 - 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Идея: dropout_loss\n",
    "\n",
    "лосс-функция по типу exploss для тренировки обычного классификатора.\n",
    "\n",
    "- оптимальным всегда оказывался вариант с p=0 \n",
    "    - на 3 миллионах выиграл у всего остального\n",
    "- назначение весов (равномерных) почему-то не такое, как везде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class DropoutLoss(BaseEstimator):\n",
    "    def __init__(self, p=0.0):\n",
    "        self.p = p\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.sample_weight = numpy.require(sample_weight, dtype='float32') \n",
    "        self.y_signed = numpy.require(2 * y - 1, dtype='float32')\n",
    "        \n",
    "        _, first_positions, self.event_indices = numpy.unique(X['group_column'].values, return_index=True, return_inverse=True)\n",
    "        # self.b_signs = numpy.array(X['signB'].values)[first_positions]\n",
    "        # track_z = - w(track) * sign(track) * sign(B)\n",
    "        self.track_z = - self.sample_weight * self.y_signed\n",
    "        \n",
    "        self.event_losses = numpy.ones(len(first_positions))\n",
    "        \n",
    "        # just in case \n",
    "        self.sample_weight **= 0.0\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def prepare_tree_params(self, pred):\n",
    "        # normally, prediction is weight1 * pred1 * sign1 + weight2 * pred2 * sign2 ...\n",
    "        # loss is exp( - isloss * decision)\n",
    "        # in case of dropout\n",
    "        track_exponents = numpy.exp(pred * self.track_z)\n",
    "        track_multipliers = self.p + (1 - self.p) * track_exponents\n",
    "        self.event_losses[:] = 1\n",
    "        numpy.multiply.at(self.event_losses, self.event_indices, track_multipliers)\n",
    "        grad = - self.event_losses[self.event_indices] / track_multipliers * track_exponents * self.track_z\n",
    "        \n",
    "        return grad, self.sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = numpy.bincount(data['group_column'], weights=logit(_p[:, 1]) * (2 * data['label'] - 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.hist(x, range=[-2, 2], bins=21);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "KFold prediction using folds column\n",
      "0.635161136287 0.658377172378 0.663936039235 0.665960318105 0.666877294068 -0.1\n",
      "None\n",
      "KFold prediction using folds column\n",
      "0.636615909766 0.65919663933 0.664267018983 0.66611973827 0.667091338142 0.0\n",
      "None\n",
      "KFold prediction using folds column\n",
      "0.63799787623 0.659486301935 0.664300595436 0.666120291879 0.667045452773 0.1\n",
      "None\n",
      "KFold prediction using folds column\n",
      "0.610217993953 0.636684892954 0.64743116016 0.651641956678 0.653798197951 -0.1\n",
      "None\n",
      "KFold prediction using folds column\n",
      "0.610174493123 0.636754846124 0.647443622341 0.651526328129 0.653749644457 0.0\n",
      "None\n",
      "KFold prediction using folds column\n",
      "0.610405353867 0.636824226617 0.647576410593 0.651747492804 0.653873407736 0.1\n"
     ]
    }
   ],
   "source": [
    "for p in [0.0]:\n",
    "    _dropout_dt = DecisionTrainClassifier(loss=DropoutLoss(p=p), n_estimators=1000, \n",
    "                                          learning_rate=0.03, n_threads=16, train_features=features)\n",
    "    _features = features + ['group_column']\n",
    "    dt_dropout = FoldingGroupClassifier(_dropout_dt, n_folds=2, group_feature='group_column', train_features=_features)\n",
    "    dt_dropout.fit(data[_features].to_pandas(), data['label'])\n",
    "    print None\n",
    "    for i, _p in enumerate(dt_dropout.staged_predict_proba(data.to_pandas()), 1):\n",
    "        if i % 2 == 0:\n",
    "            print compute_auc_with_attention(data, track_proba=_p, track_attention=numpy.zeros(len(data)) - 2),\n",
    "    print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на AdaLoss\n",
    "\n",
    "вдруг оно и просто так работает?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hep_ml.losses import AdaLossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_base_ada = DecisionTrainClassifier(loss=AdaLossFunction(), n_estimators=2000, \n",
    "                                   learning_rate=0.06, n_threads=16, train_features=features)\n",
    "dt_exploss = FoldingGroupClassifier(_base_ada, n_folds=2, group_feature='group_column', train_features=features + ['group_column'])\n",
    "_ = dt_exploss.fit(data[features + ['group_column']].to_pandas(), data['label'], \n",
    "                   sample_weight=compute_weights(data, numpy.zeros(len(data)) - 2. ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, _p in enumerate(dt_exploss.staged_predict_proba(data.to_pandas()), 1):\n",
    "    if i % 3 == 0:\n",
    "        print compute_auc_with_attention(data, track_proba=_p, track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import logit, expit\n",
    "\n",
    "print compute_auc_with_attention(data, track_proba=expit(logit(_p)) , track_attention=numpy.zeros(len(data)) - 2)\n",
    "print compute_auc_with_attention(data, track_proba=expit(logit(_p) * 2.) , track_attention=numpy.zeros(len(data)) - 2)\n",
    "print compute_auc_with_attention(data, track_proba=expit(logit(_p) * 0.5) , track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupLogLoss\n",
    "\n",
    "требуется сравнить с ExpLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class GroupLogLoss(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        self.sample_weight = numpy.require(sample_weight, dtype='float32')\n",
    "        self.y_signed = numpy.require(2 * y - 1, dtype='float32')\n",
    "        \n",
    "        _, first_positions, self.event_indices = numpy.unique(X['group_column'].values, \n",
    "                                                              return_index=True, return_inverse=True)\n",
    "        self.track_z = - self.sample_weight * self.y_signed\n",
    "        d = T.vector()\n",
    "        d_b = bincount(self.event_indices, weights=self.track_z * d)\n",
    "        self.Loss = T.sum(T.nnet.softplus(d_b)) * 2\n",
    "        self.grad = theano.function([d], -T.grad(self.Loss, d))\n",
    "        return self\n",
    "\n",
    "    def prepare_tree_params(self, pred):\n",
    "        return self.grad(pred), self.sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_base_dt_log = DecisionTrainClassifier(loss=GroupLogLoss(), n_estimators=2000, \n",
    "                                       learning_rate=0.05, n_threads=len(features), train_features=features)\n",
    "dt_logloss = FoldingGroupClassifier(_base_dt_log, n_folds=2, group_feature='group_column', \n",
    "                                    train_features=features + ['group_column'])\n",
    "\n",
    "_ = dt_logloss.fit(data[features + ['group_column']].to_pandas(), data['label'], \n",
    "#                    sample_weight=compute_weights(data, numpy.zeros(len(data)) - 2. ) \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold prediction using folds column\n",
      "0.598972140837\n",
      "0.619825794458\n",
      "0.638208229708\n",
      "0.649180258034\n",
      "0.65518004934\n",
      "0.658841436115\n",
      "0.661151485349\n",
      "0.662837189058\n",
      "0.664041256527\n",
      "0.664855213926\n",
      "0.665501497501\n",
      "0.666057203555\n",
      "0.666498396829\n",
      "0.666853495154\n",
      "0.667119081171\n",
      "0.667394152907\n",
      "0.667537496838\n",
      "0.66764924179\n",
      "0.667723081574\n",
      "0.667794033835\n"
     ]
    }
   ],
   "source": [
    "for i, _p in enumerate(dt_logloss.staged_predict_proba(data.to_pandas()), 1):\n",
    "    if i % 1 == 0:\n",
    "        print compute_auc_with_attention(data, track_proba=_p, track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 0 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# фильтрация ничего не дала\n",
    "# _n_tracks = numpy.bincount(data['group_column'])[data['group_column']] \n",
    "# _weights = (_n_tracks > 5) & (_n_tracks < 40)\n",
    "# dt_logloss_filtered = FoldingGroupClassifier(_base_dt_log, n_folds=2, group_feature='group_column', \n",
    "#                                     train_features=features + ['group_column'])\n",
    "\n",
    "# _ = dt_logloss_filtered.fit(data[features + ['group_column']].to_pandas(), data['label'], \n",
    "#                             sample_weight=_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i, _p in enumerate(dt_logloss_filtered.staged_predict_proba(data.to_pandas()), 1):\n",
    "#     if i % 2 == 0:\n",
    "#         print compute_auc_with_attention(data, track_proba=_p, track_attention=numpy.zeros(len(data)) - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Концепция) одновременного обучения бустингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# два лосса. Дообучили один - переобучили второй и обратно\n",
    "# требует метода \"дообучи\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вначале просто запускаем нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hep_ml.nnet import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn2 = FoldingGroupClassifier(MLPClassifier(layers=[30, 20], epochs=500, scaler='iron'), n_folds=2, group_feature='group_column')\n",
    "nn2.fit(data[features + ['group_column']].to_pandas(), data['label'], sample_weight=weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "track_proba = nn2.predict_proba(data.to_pandas())\n",
    "predictions = numpy.bincount(data['group_column'], weights=logit(track_proba[:, 1]) * data['signTrack'] )\n",
    "roc_auc_score(B_signs, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Для сравнения возьмем простой MLP из keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train-test splitting macnually\n",
    "\n",
    "N = len(data) // 2\n",
    "\n",
    "train = data[data['group_column'] < numpy.median(data['group_column'])]\n",
    "test = data[data['group_column'] >= numpy.median(data['group_column'])]\n",
    "_, test['group_column'] = numpy.unique(test['group_column'], return_inverse=True)\n",
    "_, train['group_column'] = numpy.unique(train['group_column'], return_inverse=True)\n",
    "B_signs_test = test.group_by('group_column')['signB'].groups.aggregate(numpy.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(len(features),)))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(200, activation='tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy', ])\n",
    "\n",
    "for _ in range(7):\n",
    "    model.fit(\n",
    "        train[features].to_pandas().values, \n",
    "        to_categorical(train['label']), \n",
    "        nb_epoch=1, verbose=2, \n",
    "        validation_data=[test[features].to_pandas().values, to_categorical(test['label'])]\n",
    "    )\n",
    "    \n",
    "    track_proba = model.predict_proba(test[features].to_pandas().values, verbose=2)\n",
    "    predictions = numpy.bincount(test['group_column'], \n",
    "                                 weights=(logit(track_proba[:, 1]) * test['signTrack']))\n",
    "    print roc_auc_score(B_signs_test, predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы про нейросетки\n",
    "\n",
    "не особенно тянут. Выше 0.612 не получилось ни с теми, ни с другими.\n",
    "\n",
    "После iron transform получилось 0.622"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO хотелось бы одновременно обучать две нейросетки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hep_ml.nnet import MLPClassifier, MLPBase, AbstractNeuralNetworkBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AbstractNeuralNetworkBase??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(self):\n",
    "    activation = lambda x: x\n",
    "    for i, layer in list(enumerate(self.layers_))[1:]:\n",
    "        W = self._create_matrix_parameter('W' + str(i), self.layers_[i - 1], self.layers_[i])\n",
    "        # act=activation and W_=W are tricks to avoid lambda-capturing\n",
    "        if i == 1:\n",
    "            activation = lambda x, act=activation, W_=W: T.dot(act(x), W_)\n",
    "        else:\n",
    "            activation = lambda x, act=activation, W_=W: T.dot(T.tanh(act(x)), W_)\n",
    "\n",
    "    return activation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
